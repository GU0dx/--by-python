{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e3afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_mnist\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a5c90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (60000, 784)\n",
      "The shape of Y_train is: (60000, 10)\n",
      "The shape of X_test is: (10000, 784)\n",
      "The shape of Y_test is: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#加载训练集或测试集\n",
    "path = './MNIST Data' #数据集文件所在目录\n",
    "# 加载训练集合测试集\n",
    "# 设置normalization为True，将数据缩放到[0,1]之间\n",
    "# 设置one_hot_label为True，将标签转化为one_hot向量\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist(path, normalize=True, one_hot_label=True)\n",
    "print('The shape of X_train is:',x_train.shape)\n",
    "print('The shape of Y_train is:',y_train.shape)\n",
    "print('The shape of X_test is:',x_test.shape)\n",
    "print('The shape of Y_test is:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3577b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABqCAYAAAClIwp2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtq0lEQVR4nO2dWYxc13mgv1v71rVXV+8r1xZXkaIpWqJkKV5iGPAgghIZwsRJDOhlBpjA8xBh5mFeMwMkyDzkxUCC8WRxNEaiWFIUSY4sWSZFMRTFRaRINnup3qu7tq59rzsP5DmqbpLi0l1Lk/cDGuyuqmaf+997//uff1VUVUVDQ0NDY+uha/UCNDQ0NDQeDE2Ba2hoaGxRNAWuoaGhsUXRFLiGhobGFkVT4BoaGhpbFE2Ba2hoaGxRNqTAFUX5jqIo1xRFmVAU5dXNWpTGDTT5Ng5Nto1Dk23zUB40D1xRFD0wDnwTmAfOAD9QVfWLzVveo4sm38ahybZxaLJtLoYN/O4RYEJV1SkARVH+Afg+cMcT5ff71aGhoQ38yYefs2fPRlVVDXCf8tVke3ceVLagyfduhEIhotGogibbhlB37a5hIwq8F5ir+3ke+Nr6DymK8grwCsDAwACffvrpBv7kw4+iKDM3v72rfDXZ3h/3I9ubn9fke48cPnxYfKvJtgHUXbtr2IgPXLnNa7f4Y1RV/YmqqodVVT0cCNzyANG4M3eVrybbB0a7dhuHJtsmshEFPg/01/3cByxubDkadWjybRyabBuHJtsmshEFfgbYrijKsKIoJuAl4I3NWZYGmnwbiSbbxqHJtok8sA9cVdWKoij/GXgX0AN/rarq5U1b2SOOJt/Gocm2cWiybS4bCWKiqurbwNubtBaNdWjybRyabBuHJtvmoVViamhoaGxRNAWuoaGhsUXZkAvlYUJUpKqqSq1WA6BWq6GqKoqirPnS6W489xTldhlTGhoaGs1BU+DcUNSFQoFKpUIymWRxcZFCoUAoFGJ5eRmn08n27dux2WwEg0H8fj96vR6z2YzBoIlQQ0OjNWjahy8VeDabZXp6mg8++IB4PM7Jkye5fPkyPT09PPfcc3R2dnL06FH27duH1WrFYDBoClxDQ6NlPNLap1arUa1WKZVKLC8vE41GuX79OuFwmGQyyerqKqVSiVwuRzwep1qtMjExgcvlwul0YrPZMJvNrT6Mhw5VValUKtRqNcrlMplMBoCOjg7MZrN0Y2kurLtTrVYpFovUajWKxSK5XG7N+zabDbvdjk6nQ6/Xo9frW7RSjQfhkVXgqqpSLBbJ5/NEIhH+9m//lgsXLhCLxVhaWqJUKpFOpwFIJpOcPHkSk8nE+fPnef/99xkZGeHHP/4xDoejxUfy8FGtVkmlUmSzWcbHx/nFL35BtVrlpZdeYv/+/RgMBiwWi6Zs7oF8Ps/Vq1dJJBKcPn2as2fPUi6XZZzn2LFjvPjiizidTtxuN1artcUr1rgfHlkFDjcURT6fZ3V1levXr3Pq1ClKpRKFQkEGNRVFoVQqEY1GAYjH48zOzpLP52+xZjQ2B1VVKRQKZDIZJiYmOHnyJOVymaNHj7J7924URZHnR+POiJ1MOBxmZmaGzz//nHfeeYdisSjl19HRwTe/+U2MRiMdHR0tXrHG/fLIKXDhMqlUKkxNTXHu3Dnm5+eZnp6mWCxSrVblxS22lPUZKADFYpFMJkMikSCTyWAwGDCZTPL9dt3aq6pKqVSiXC6jqirVahUAs9mMxWJpm3XXajWy2SyxWIxSqUQwGATAbre3eGVbg1qtRi6XI5/Ps7CwwOnTp5mfn2dqaoparbbmPLfLOdd4MB45BV6pVEilUuRyOX71q1/x2muvkUwmWVpaolAoyM8pioLRaJSK2WAwoCiKvDFWV1dZWlqir6+Pjo4O9Ho9Op1Ophi2K4VCgXg8TqVSoVAoUK1W6erqwmQytY1LQlVVEomE3Ol0dXVhMBhwOp2tXtqWoFarEYvFmJ2d5dKlS/zyl79kcnKSQqFwiwLX2No89Aq8Pr9bVVXK5TKpVIpUKkUsFiMSiZDL5SiVSmu25UJpm0wmqTz0ej3RaJR8Pk+5XGZlZYVoNEq1WpWfa+fMFHH8Yv25XI5KpYLX62310tYg1inOi8b9I+SXzWZJp9Pk83m546rnUVfmotZD7EjF9/W1IOsxGAzS2BHyE7+jKMqaXXujDbr21DSbhBCqSBMsFAosLCzw5ptvEg6HuXTpEvF4nHK5TLlcXvO7Op0Op9OJz+djYGCAAwcO4HA4+Pjjj3nnnXeIx+O89tprfPTRR+zcuZNvfOMbuFwuhoeHcbvdrTngu6CqKqlUilAoRC6XY2VlhWKxiM1mo7u7uy0scHHO0uk00WiUeDxOPB7HZDKRz+flDaZxZ1RVJZ/PE41GSSaTFAoF6TbT+JJarUY+n6dYLJJOp5mfnyeXy7G0tCTvjUQiQbFYlL9jNps5evQo27Ztk8F0nU5HMpkkkUhgMpno7++XGVNWq7WhSvyRUeD5fJ5UKsXs7CynTp3i2rVrrK6ukk6nb3th63Q6rFYrHo+HYDDIoUOH8Hg8zM3NodPpSKfTnDhxAoAnn3wSp9NJb28vfr8fl8vVtpaN8Itms1mWl5cpFAqkUqm2urlVVSWbzZLJZMhkMqRSKSwWyy0PWY07Uy6XpeUtUjI11iJiQqlUikgkwpkzZ4jH48zMzDAxMUE6nWZ2dlZmowE4nU65g7XZbHg8HnQ6HTMzM0xOTtLR0YHJZEJVVZxOJxaLpaHH8FAr8Hw+Tzwep1AoMD8/z/z8PBMTE4TDYXkS7kS1WiWbzRKNRunq6pKRe7PZTCAQkAqmVCrJJ/jq6uptt6ntgqqq8qIUwcx2oj4vf35+nlAoRLlcxuFw0NHRgdvtxmKxyHiExloqlQrlcplisSiv9aWlJWlBmkwmaRXu2rULv9/PoUOH8Hq92O32ttiBNYNyuSyv/+npaWZnZ1laWmJiYoJkMkk4HCYajVKpVHA4HFitViqVCtVqFYvFQiQS4cKFC5hMJhwOBzqdjpWVFVZWVujo6KCrq4tSqURPT4+MjzWKh1qBRyIR/uVf/oWVlRXOnz/P5cuXKRQKJJPJW3ze6xGBoNXVVdxuN+l0Go/Hg8vlYv/+/cRiMa5evUqpVCKTybC8vCzT39oVVVUJhUKcPHkSs9lMX19fW+WxCwWUTCY5f/48//qv/8rg4CAHDhzA7/cTCASw2WxaEc8dKBaLxGIxkskkJ06c4O233yaXy0kLsqOjA5/Px7Zt2/jDP/xDxsbGcDgcuN1u9Hp928ZuNpt8Ps/8/DyJRIJ//ud/5vTp06TTaek2qVQqlEolrFYrIyMj+Hw+uYOvVCp8+umnnDx5co2/u1QqUSqVcLlcZLNZent7OX78OD09PRiNxoYdy0N5xoSfNJ/Ps7i4SDgcZnx8nFAotCa/u76iT/iphBUIN57UlUpFWt+KomCxWHA6ndRqNXliRLWbqHhrV1RVlVWlVquV7u7uVi9pDaLyslAoyB1NMBjEYrFgs9lkpoymvNcirulKpUImkyGZTBKPx2UaZqVSQVEUTCYTbrcbl8tFb2+vzO4xGo1r0mQfVoReKJVKrK6uEolEpOUtKq4rlYrMKDMajfh8PgKBAIVCAbPZTKFQIJFIEI1GpYu23hCs1Wokk0nsdjvZbLbhx/TQKfD6xlQLCwuMj4+zuLhIIpG4xeL2er1y+zg0NITNZpO54cViEYvFgslkIhAI0NvbS2dnJ3v37sVms7G8vMzU1BTLy8trItlbgXZcb61WY3l5mQsXLhAKhZiZuTGE22Kx4Ha78fl8mEymFq+y/ahWqzJIOTU1xbvvvkskEuHy5cvkcrk1HTW3b9/O0aNH6enpobOzE7PZLEvo4eHMSBHXuDCyKpUK169f5/XXXycWi3Hp0iUymYzckVosFsbGxhgaGqKjo4Pdu3fj9/tlVk8mk+GDDz7gzJkzpFIpwuHwml230WjE6/XS19eH1+ttuEzvqsAVRflr4HvAiqqqe26+5gVeA4aAEPC7qqomGrfMe0dUV+ZyOaanp7l48SKxWOyWqklFUXC73ezYsQO3283evXvp7Ozkk08+4fLly5RKJcxms9xi+nw+PB4PJpMJr9fL3NwcH3zwwYaV4B/90R/x1ltv0dnZyaVLl8TaGibfdlPc9SwsLPDWW28Ri8VYXLwxB9diseDxePB4PPe9FW22bFtBrVaTwd7z58/zxhtvEA6HWV1dXaNYDAYDAwMDPPnkkwQCATwez4b6+Gwl2YoUwUwmQzab5dy5c7z++usyPlYqlbDZbPT39+Pz+Xjqqad4+umncTgcBINBzGaztLaLxSLlcplsNks4HCaRSKyRs16vx+v10tPTg9PpbLgCv5f8lv8DfGfda68C76uquh14/+bPLaVWq0nf1crKCrOzs7ItrAhAwA0Bi4DYyMgI27dvZ3h4GL/fj9PpxOPx0NfXx8DAAHv27OGJJ55gZGREpgXZbDZcLhder1dGmEXEX/TvqFQqVCqVe1KUf/AHf8A777yz/uVNla8oqa539dwtBtBsxE0mgsLCFWWxWOT5ut/gZTNk22pE1erq6iqJRIJUKiUD9KqqotPpsNvt8toOBAK4XK4N+7u3imyFS06kB87MzLCwsCBlZDabcTqddHZ2sm3bNkZHRxkcHMTpdGK329fkfAt3ajqdlg+D+txvk8kkDY6uri5sNlvrLXBVVT9SFGVo3cvfB569+f1PgQ+BP9nMhd0Pt2tM9cUXXzAzM0M8HqdUKkmF4HQ6ZQT+O9/5Dr/1W78lszOKxSIHDhyQPsHHH3+cgYEBeYJFvwhVVTGZTHg8HhRFIR6P8+mnnxIMBjl+/DhDQ0NS2d/tRjl+/DihUGj9y5sq30qlQi6XI5fLEYlESCaTa3z97YLYPWWzWbk2n8/H2NgYgUDgvkvpmyHbVlOpVJienuby5ct88cUXrKyskMlkpAFht9sZGRnB6/Vy8OBBdu3ahclk2rA7aivItlarkUqlWFpaYnl5mZ///OfSpbq6uoqiKPT09NDT08PevXt56aWX5ANOKF9hNBQKBRYXF4nH41y8eJF///d/lzoHbhgaDoeDwcFBDh06JF2tjc7sedDHcFBV1SUAVVWXFEXpvNMHFUV5BXgFYGBg4AH/3N0RvsDV1VWuXr3KRx99dNvGVGazmWAwiNfrZefOnfT19VGtVlleXiaVStHZ2UmpVMJgMDA2NkZ3dzc6nQ6TybQmId/hcMgy+2KxSDQapVarEY/HyefzKIqykc5u9yTfe5VtrVaTssjlctK/304WOHwZQK4vOjGZTHR0dOBwODbrZmi7a3cj1Go1VlZWWFhYIBaLkc/n11Svii19IBCgu7tbWpUNou1kKwqapqamOHPmDBcvXpTvifhKMBikt7eXoaEhuTtZf61VKhUZ+BQV3PUYjUaZ4ODxeGTldsst8I2iqupPgJ8AHD58eFM1hnCbVKtVIpEIs7OzTExMsLy8LF0Eon+0yWTCaDQyMDDA6OgonZ2deL1emYUiWmkKf7dOp8Pj8ciI9FedCLH9LxaLzMzMMD4+Li3GRqYQ3atsK5WKTIlcXV2V2TXtoMCF7KrVKul0mng8TjKZpFKpADd8t1arVQbcmry2hl27G6W+wnhycpKrV68yMzOzZktvMBjo7Oxk3759dHd309nZ2TaBykbKVhhyuVyO8+fPc/r0aal44caurre3F5/Px9GjR9m1axfDw8NYrdY7Kt10Os1nn31GOBxmYWFhzXs6nY5t27Zx4MAB+vv78Xq9GAyGplyvD6rAlxVF6b75lO0GVjZzUfeK2HKXSiUmJib48MMPWV5eZn5+nnw+j8VikQUKbrcbu93Ojh07eOKJJ+QFLbZIRqNRBvhE1L5ecX/VhS/S37LZLFeuXAHgscceY2ho6EGt8E2Vr3iwLCwsEI1GKZVKa7outhLxEC6Xy8RiMRmAE1ak0WjEZrPJm2sTaItrd6OIh14mk+GLL77g3/7t36QsdTodZrMZs9nMyMgIzzzzDAMDA3R3dzdagbeFbHO5HGfOnGF+fp7f/OY3vPfee3IHCtDb28szzzxDZ2cnv/3bv83o6KgckXgnYy0ajfLrX/+amZkZmSEl0Ov17N27lxdffFHudoTh1g5BzNvxBvDDm9//EPjF5izn/hAKXCThR6NRUqmUvPktFgs+n49gMEhfXx+Dg4MyAi+s4/p8cGG1GI1G+QS91/xYofhFhWY2m92IgtxU+dZn5tT3gtbr9RiNxpYWxtRPislms7JDovA/ivOwidZMW1y7D4rIhhAGQzKZJJ1Oy2wKYXzYbDacTiculwu32y1LvBt8nlsmWxGoL5VKZLNZlpaWCIfDRCIRGdgVuzm/309nZyfBYBCXy4XZbL7tfSB2h6JzZzKZlEWAcOP+EXJ2u923uE6acU/dSxrhz7gRmPArijIP/A/gT4H/pyjKj4BZ4MVGLnI9Qlmm02lOnjzJ4uIi586d48yZM1JJWa1WHn/8cZ5//nlZuODxeLDZbHi9Xhkx3kxEE6ZEIkE6nb6nop4f/OAHfPjhh0SjUfr6+gD8bLJ8K5WKLPUVvV8MBgM+n4/u7u6WVmPm83nOnz/PwsIC586dI5FIUKlUpMJxu90YjUbpyrofmiHbZiLiA5VKhatXr/Kb3/yGpaUlxsfH13zOZDJx4MABtm3bxu7duxkeHpZpmJulVNbL9qZfvWWyrVarLC4usry8zJUrV3jvvfeYm5tjaWkJRVFkrYfH4+HrX/863/ve92SzuvUKV+iXSqVCOp0ml8sxMTHBwsKCTD1UVVWmHPp8Pp555hlGRkZkC+pmcS9ZKD+4w1vPb/Ja7pn6jnWfffYZs7OzXLhwgWvXrgHIApzBwUGOHz+Ox+PB7/djt9vXDDLYzAtaIEqXRRHF3fjZz3625mdFUaKqqsbYRPlWKhVpPYhtpMioEemQrbLA8/k8n332GdPT04yPj5PJZNDpdLIpmMPheOBdQjNk20xEm91SqcTly5d59913iUajtwTUDAYDvb29PPbYY+zevVvuODeT9bI9fPgwoVCoZbKtVqssLS1x7tw5rly5wokTJ4hEInIoi8ViYWBgAL/fz2OPPcbIyIh0maw3DIR+qVarMm60tLREMpkkk8nIHkIul4udO3fS29vLtm3bpPXdTLZcJaa4iEXQa2VlhXA4LC1Lh8PBY489RjAYZNeuXfh8Pux2uzxZYosJtP3whUai1+txOBxyN9JI6i0a0RlPtPGcn59ndnZWFkWIFM3h4WF6enro7e1tuZunXRAN1nK5HNFolGg0KoPS8GXOvKhl2LFjh7QwH1bqe3nHYjFCoRCRSIRyuYyiKDgcDpxOJ11dXezdu1dmm9zO6q5Wq7IwKpFIkM1mmZiYYGVlhampKdkUzOVyYbVaGR0dlXnjDoejJdfnllPg4ubPZDKEQiHOnTvH5OSk9O0GAgF+7/d+j/3799Pb20tvb+8dM0keZYVgtVrp6emRvYsbJYv6G0NUr2WzWS5fvszU1BRLS0ucOHGCeDwuCyM6Ojo4duwY+/fvZ+fOnZjN5kem0dJXUS6XZS7y5OQk09PT0vcN4PF4GBsbk/UIhw4dkjGdhxVhKRcKBc6ePcu7775LMpkkn8+j0+no7u6WpfEvv/wyvb29Mg++XoGL/6NUKnH69GnefvttUqkU165dIxKJyGIpg8HAtm3bGBkZYWxsjOPHj+P3+5sRX7gtW/KuEFNlRPVZOp2WgUibzUZvby89PT2y9L2Rlna9Rb+VEI25zGbzPeWr3i4gW//a+vfFz8LyFnMaV1dXyWQyzM3NMTMzQzQaJRaLyU5vcMO9I6rZRLvOrSjjzWJ9s6pIJCJ7fQvrW6fTyaC9x+ORrXcfJHaw1RBKPJ1Os7y8LHueKIoiq3j9fr8MMtZnm4leMaLKUrSenpiYIB6PyxazgnrXo8/nw+l0yqENmgK/B8Qw4rNnz3L16lXZ8cvr9eJ2u9m1a5fMNmn0U7G+r4iiKG3diXA9ovXt+kHOt/ucmFhU/xnRMVAE1upL82u1GpFIhIWFBVlEJGIWYgJSLpdbkw2j1+tlbrPBYJDtY+12+0OvgL6K+gHFc3Nz/PKXv2RhYYErV65QrVZl8ySbzcbXv/51vv3tb8vClEehb/r6ZlW5XE5eq0ajkT179vC9732Pzs5OLBYLpVJJzgkolUqy7iCdTstBJ5cvX2Z8fFxO8arHaDQyODjI/v372b59+x396M1iyynwarXKuXPnePfdd1lcXCSTyaAoCl6vlx07djAwMEAwGJQDcJtxAa+3RLeCIhdbRmGtfBWi14s4LlVViUajTE9Py/a0Imgr/NyXLl3ik08+ke4TQLboNBqNbN++nWAwKHdOBoNBWlKiA6TX68VqtT70SuirqNVqrK6uyp726wcUW61W+vr6CAQCHDx4kGeffVZWCT8Kbidxv4mipkwmI9/T6/Xs3LmTY8eOydRgUdR2/vx5EokEn3/+OXNzcyQSCaampmS6bX0Fdz0Gg4G+vj727dsnO2S2MsawZc6wKFwoFoukUikZZBAuDLvdLvNdW2l5mM1m7HZ7y3xit0MoyXoroVgssry8zNzcnCxiuN2FqKoqq6urxOPxNQp8ZWWF6elpisUimUxG9oRQFEX6vW02G7VaTV7kImfWYrHQ398vfYcWi4VcLrcmuGw2m1t+c7SS+sBvIpFgenqa+fl5ksnkml2TaA8hip1EvOBR2bXU+7HFTq5eqcdiMZaWltZcSwsLC4RCITlKTbik6o0ZkfAgrnlRl2A0GmVgtB2Miy2jwAuFArFYTPqlrl+/Lsci6fV6hoaG2LdvH8PDww2fQyeov3jECe7q6qKvr082y28HRKWpKIxRFIVYLMZbb73Fxx9/TGdn5x3XW61WCYfDhMPhNRe0sHbqrRS73U5XVxdGoxG3283zzz+P2Wymq6sLu91OIBCgv78fk8kkI/lffPEFk5OTpNNpWUUo3hfuk1bfJK1AVKdmMhnefvtt3nzzTRKJBAsLC9L6Ft0G/X6/dDnVF6U8Koh7z2q14nQ6pYuuVCrx+uuvc+nSpTWxgFQqJV0oIuVXFPHZ7XYsFovcTYq0QVER7PP52L59OwMDA3Ln2EraQ8PcA2LUlvBZpVIp+ZQ1Go3Y7Xa6u7vx+/1NtdrqFbjYCYj85XZRPKI3hvhSFIVcLse1a9cwGAyy2+Lt5FatVpmbm5MKXChsYdELC1D8P+JcuN1uent7sdvtjI6O4na7ZUMl0XrTYDCQy+VwOp1rdghiR/AwZ0/cDWF95/N5JicnOXXq1G0/J3YrNptNto14lHYt4r4TSlz0OFcUhWq1ysTEBBMTE2s+fzssFovsYWIwGOTknvprUuwWRd+kdri/t4wChy/dKKK8VSToW61WgsGgVBSNeCrWl8onk0nZkhKQ06ldLhdjY2M89dRTa/ohtBqHw8ETTzzBtm3bpHIUI6SEq0Mo0fWIh6TIWAkGg9jtdqxWq2yXK/rMiIvbZDLJqL/ZbJblyvWuJdHDZmlpSVa36XQ6Ojo6WlIQ0W7k83lmZ2dlrrdwTa3HZrMxOjrKY489JjtnPkoI96DFYuGJJ56QbV9PnDhBKpWSNSM6nQ6r1YrRaMTpdOL3+2UqbUdHh+wkqCgKoVCIa9euyZYYxWIRp9Mpi3baaY7sllHg9a1GRQdCg8GAw+HA4XAwNDTE6OhoQyy3ep9aOBzm1KlTLCwsMDc3B9zoMT42NkZnZyfPPfcchw8floqyHXA4HBw4cIBqtYrf76enp4dUKkUoFJJB4K+68b1eL6Ojo3R0dLB3714CgQA+nw+v14vZbJbZIvXWEKy1juq/RNXc7Ows586dY3FxkXQ6jdvtlt0gW701bTWJRIKPPvqIcDjMzMzMHa09p9PJkSNHOHjwYMMnoLcjYhdosVg4duwYO3fu5OzZs0xPT8sUS5GtIypSd+7cya5du3C73Xzta1+TGTuixfJ7770n29BGo1EAgsEgBw8epKurC5fL1RbWN2whBQ5fKtJ6S0RkNojy+UYEMOsn2iSTSZaWlohEIhSLRfn0F2mMoodHO/luhV+5Wq3KvjB2u518Po/NZrvn/8fpdNLT00MgEJAVfyIv9n66LorJO6urq9L3LfqzCGuoXWTXbMT1XSwWicfjrK6urgkQC0TPahFQu5fhIQ8r9TUg1WqV7u5uBgcHpQ87n89jNpvp6enB4XDQ1dVFd3f3mvzw+kwo4QOvT681mUzY7fZG91O/b9pnJQ9AfWRYbM83MwIv3Ca5XI6ZmRlWV1f58MMP+dWvfiU7nHm9Xvbu3ctTTz1FT08PwWCwqd3I7gedTicDipVKhWPHjt01hbAeEYEXchayvt8dj6qqxGIxLl++zPz8PIVCAUVR6Ovr4+DBg3LA9KOGSO2sH8i9vLxMPB5f8zlFUejq6mJoaIgdO3YQDAbl9Jd2u+aahXCRmEwmDh48yKuvvko+n5cuVxErMBgM2Gw2ma3mdDoxmUxUKhXZDXNmZoaLFy/KtgWqqko3qc/naxvXKDwEClw8OS0Wi+xat1kIBZ7P57l27RozMzOcP3+eU6dOoaoqHo9HDj7dt2+fnKvZrttY0Rui1T48VVVJJBIsLi4Si8Vk34pAIMDIyAh9fX2P5AR6YXmL+Y3T09OEw+HbDuT2+/2MjIzIkX/NyrxqV0QwHW7EBTo7O28bM7jTA06U0WezWVZWVpifn6dUKklDzGQyyfa87XR/b2kFLlLUgsHgpiklkT4keq7k83nC4TDXrl0jHA6Tz+cJBAJYLBYOHjyI3+9nz5490g/cTturrUZHRwednZ1y/uijRqVSYX5+npWVFcbHx2X3yNtNJ9qxYwd79uxhcHDwkVfed+J+dyMiznU7xe9wOOjp6cHv97dNbAu2uAJ3uVzs3buXYDCIz+fblO2jaJRfKBT4/PPPuXTpEgsLC7L3cU9PD0eOHKG7u5uXX36ZkZERTCaTLKltp6fzVkKn0xEMBjlw4AAul+uRtMBzuRw///nP+eSTT1hcXGRhYYFSqSQVuM1mY3BwEJ/Px3PPPcd3v/tdrFbrpreKfRSpH95Qq9XWDHNRFAWfz8euXbvweDwbmXW76WxpBS4CCw6HY8OWb33lm5iMPjc3x/T0NMvLyywuLpJMJunu7sbj8cgpP4FAYJOORkOkGormQI8apVKJhYUFPv/8c+lKqbcGRYGU1+uVzdoetbzvZlCfRSUQufZ3qlhuFVtOgdd3uUulUkxPT5NKpdZ0DLtfyuUyq6urZLNZFhYWOHv2LMlkkvHxcaanp1FVleHhYYxGI8888wzPPfccbrcbl8u1WYeloQHcaHGQzWZv22DM5XKxf/9+mUVxLwO3Ne4NMaFKBCu3ikzvZaRaP/B/gS6gBvxEVdX/rSiKF3gNGAJCwO+qqppo3FK/VN6inDuVSnHx4kV8Ph+xWOyBZ1BWKhVCoRAzMzOcOXOGN998U7Y9LRQK9PT08PTTTxMIBPjWt77FkSNHNiXbZW5ujt///d8nHA6j0+l45ZVXAGiFbB9G1ssX6IT2lW/9fNDbWYF+v58DBw4wPDwsK2dbtVNZL9tUKiXW25ayvRuiwMdqtW6aO7YZ3MvZrwD/VVXV3cBR4D8pijIGvAq8r6rqduD9mz83lfoLPhqNyrabwm8oOuGJAiDR7zeTyZBOp0kmkyQSCSKRiOxPHYvFZPqRSJsT+aIi/1nk4G70JBsMBv7sz/6MK1eu8Mknn/CXf/mXABbaQLatQPggRV74RlkvX6CzXa5dgSjZzmQypFIp6Tap32mKL5PJREdHhxwz10ols162kUiEdpPt/SCSF0SNwlbhXmZiLgFLN79PK4pyBegFvs+NYccAPwU+BP6kIau8iQgoCKtDtDktl8ucOXNGKtrh4WEcDodsUFOr1eTNsbq6yrVr12RflUQiQS6XY3p6mpWVFYrFoky36+3txefz0d/fzze+8Q3Z9Gmzbpzu7m66u7uBGxkYu3fv5vr16yZaINtWIgq0stks8XgcVVXx+/0b/n/XyxfI06Jr906oqsrc3JycCxoKhe748LLb7bJRWqvz5NfL1mKxUCwW20q290O9GzUajW6KAdEM7ssHrijKEHAQOA0Ebyp3VFVdUhSl8w6/8wrwCsDAwMCGFrvu/5UWuKqqzM/Pc+HCBYLBIG63W1Yf1g9AjsfjzM/Pc+bMGeLxOAsLC8zMzFAqlYjFYhQKhTWDfoPBIMFgkP7+fgYGBuRU+0ZYPmI8HJABRlop22ZS3ztd9GexWq2b3lM9FAoB2GiDa7ce0Vv97NmzLC8vy1hO/aAQgah6FU2r2mWbHwqFRK56W8n2fhCzMJPJpGxTvRW4ZwWuKIoD+Efgj1VVTd3rxaOq6k+AnwAcPnx406Ui0n/m5uaw2+2Ew2FqtRqBQECmWJXLZUKhENFoVM4TTCaTRKNRebJEkn69Bb9v3z62b99OIBDA7/djs9kakp+cyWR44YUX+Iu/+AteeOGFe9ZcjZZtM7jdRPDNRsgXmGuXa1e4i8rlMisrK7I9g9i+i+Ck0Wikq6sLn8/H8PCwLJlvlywdIdv+/n4mJyfbQrYPghgIkc1mHy4XCoCiKEZuKO+/U1X1n26+vKwoSvfNp2w3sNKoRX4VYuTX+Pg4oVAIs9nMxx9/jM1mw2KxyLJxMXWjUqnIEV/C72Wz2RgeHsbr9dLf38/o6Cher5fnn3+eoaEh2Y5VuG820/Ipl8u88MILvPzyy/zO7/yOeLktZNss6tsO1Pt/N4N6+X722WerN19uuXzFhPlsNsv4+Djnzp2TvanrB3DYbDaOHDnC9u3b2bNnj8yRbwcFXi/bv//7vxcvt1y2D0K1WiWVShGLxeRw7a3AXa8C5cad9VfAFVVV/7zurTeAH978/ofALzZ/eWvWIfueiK/6i1gEgsT4KdEtcGZmhrm5OZaXl2Vrznw+L10vJpNJtqMVjZoCgYAcyybaqG5W4LIeVVX50Y9+xO7du/nxj39c/1ZTZdsu1Go12f5zM5R4O8u3ftZoPp+XGU+1Wm1Nf2sx3KKrq4tAINA2fXbaWbYboR1kez/ciwX+deA/Ap8rinL+5mv/DfhT4P8pivIjYBZ4sSErvInZbKazsxODwUB/fz9DQ0Nks1kikYhsGwlf+rJEj5RUKiUH+MLaYQEDAwOMjIzg9/v51re+xcDAAFarVUb53W53Iw+JkydP8jd/8zfs3buXAwcOiJddNFm27YCqqiSTSSYnJymXy+zYsWPD/+dt5DumKMp3aQP5CgUu+m+k0+k1ZfOi90YwGOTo0aM8++yz2O12WfHbaiWzXrbj4+O0i2wfBKPRSDAYlD3t4fZxiHbjXrJQTgB3OoLnN3c5d0a0GgXo6uqiv7+fSCQip5wLhC9LsF74QoGbzWa2bdvG2NgYAwMDHD9+/JbMh0afuKeeeuoWS1NRlKSqqjGaKNt2QMzeDIfDmEymNef0QVkvX0VRvlBV9e2bP7ZUviJ2UyqV1vhe69uXOhwOAoEAe/bsYXh4uK0UyXrZHj58mE8//bQtZPsg6PV6nE6nnDEAyJmj7RzQ3DKVmGJLabFYGB0dlc3W9Xo9q6ur8ms9qqqi1+tlM3eXy8Xg4CA2m40dO3awa9cuuru7MZvNbXWDPIq0843SKup7cmg0DtFxsFaryb7fOp1O9kapHyajldI/ACLyrtfreeaZZ3j88ccJh8OcPHmSaDTKJ598wqlTp6hWq7f8rtls5siRI4yOjtLb28vRo0dxOp14vV7ZHrKdOow9amx24FJD434RfcJNJhO9vb10d3fL1ONCoUA+nyeVSsnaknahfVZyF0QmiJibaLVa0ev1DA8PY7VamZqawmw23xIAE+OWfD4fvb29DA8PMzAwQEdHh2xOo1k3zUezLG+wftyc9iBrDWKHDzcKppxOJ5VKRSZKVKtVisWizGAT7pVWs2UUeD1ibJrH4+HAgQPk83kGBwc5duzYLdac2Brt2rULn8+3ZhRYOxVDPEqI9pyjo6OyEf+jdh50Op1sA9vV1cXg4CCZTEYWlGm0BjEZ6umnnyYSifDBBx+Qy+VYXFzk5MmTdHd3y9mYer3+lmy4ZrPlFLh4Uur1etlOFmDXrl1fab3UR+6FwB81pdEu6HQ6fD4fY2NjlEqlR3IggV6vx2q1YjAY6OvrY+fOnUQiEdmLXqM1KIpCT08Pzz//PFNTU1y6dIn5+XkmJyf59a9/vWb2rWgtqynwB0AoXxFQaKfAgsbdMZvNuN1uuru72bNnDx0dHYyOjsp854d9Io9wCdZqNTo7OxkcHJTDdROJBC6XS9YktLrvyaOEqH6tH3gs5B+Px9Hr9UQiEZLJJA6HQ06yb5UxuGUVuMbWRa/XEwgEcLlcDA8Pc/jwYYrFIg6HA7vdLrtAPsyIYdB6vZ5Dhw4xMjJCtVqVnTDF9lyU0ms0D5fLxbZt23A6nezdu5disUgymeTatWuEQiFcLhcrKyvs3LmTJ598UlbNtsIS1xS4RtMRgWWLxYLT6aSz87b9jh5qhAUuUlw9Hk+rl6TBlxa4w+GgUqnQ1dVFb28vxWKRROJGW/OJiQnghiHy+OOPY7VaWxZ81hS4hoaGRh06nQ5VVbFarRw6dAiHwyHbSItq2dnZWQKBALlcTg4zb4UbV1PgGhoaGnWIhAfRSOzgwYNcv34dp9NJLBbjzJkzXLhwAbvdTiKRwOFwYLPZWuIL1xS4hoaGxjqEi0v0TfJ6vQSDQXQ6HR6PRza4g9YWomkKXENDQ+MOiDTBYDDIt7/9bUqlEs8++yzxeFzm74uiwlZkomgKXENDQ+M21FcKi1mkADt37pTuklbXkmgKXENDQ+MeqFfWrVbcAqWZvhtFUSJAFog27Y82Hj+bezyDqqoG7veXbsp2pgHraTWbeTwPJFvQrt17QJPtWpqiF5qqwAEURflUVdXDTf2jDaTdjqfd1rNR2ul42mktm0E7HU87rWUzaNbxtH6wnoaGhobGA6EpcA0NDY0tSisU+E9a8DcbSbsdT7utZ6O00/G001o2g3Y6nnZay2bQlONpug9cQ0NDQ2Nz0FwoGhoaGlsUTYFraGhobFGaqsAVRfmOoijXFEWZUBTl1Wb+7c1AUZR+RVE+UBTliqIolxVF+S83X/cqivJLRVGu3/y36b1BNdk2dG2abBu7Pk2+D4poxNLoL0APTAIjgAm4AIw16+9v0jF0A4/f/L4DGAfGgP8FvHrz9VeB/9nkdWmy1WS75WSryXfjX820wI8AE6qqTqmqWgL+Afh+E//+hlFVdUlV1c9ufp8GrgC93DiOn9782E+B/9DkpWmybRyabBuLJt8N0EwF3gvM1f08f/O1LYmiKEPAQeA0EFRVdQlunEyg2SNmNNk2Dk22jUWT7wZopgK/XfeXLZnDqCiKA/hH4I9VVU21ej1osm0kmmwbiybfDdBMBT4P9Nf93AcsNvHvbwqKohi5cZL+TlXVf7r58rKiKN033+8GVpq8LE22jUOTbWPR5LsBmqnAzwDbFUUZVhTFBLwEvNHEv79hlBs9JP8KuKKq6p/XvfUG8MOb3/8Q+EWTl6bJtnFosm0smnw3QpOjtd/lRoR2EvjvrY4eP8D6n+LG9u4icP7m13cBH/A+cP3mv94WrE2TrSbbLSdbTb4b+9JK6TU0NDS2KFolpoaGhsYWRVPgGhoaGlsUTYFraGhobFE0Ba6hoaGxRdEUuIaGhsYWRVPgGhoaGlsUTYFraGhobFH+P2IdZ488vEpWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot 标签： [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "对应的实际标签： 0 4 1 9\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(141)\n",
    "ax1.imshow(x_train[1,:].reshape(28, 28), cmap='Greys')\n",
    "ax2 = fig.add_subplot(142)\n",
    "ax2.imshow(x_train[2,:].reshape(28,28), cmap='Greys')\n",
    "ax3 = fig.add_subplot(143)\n",
    "ax3.imshow(x_train[3,:].reshape(28,28), cmap='Greys')\n",
    "ax4 = fig.add_subplot(144)\n",
    "ax4.imshow(x_train[4,:].reshape(28,28), cmap='Greys')\n",
    "plt.show()\n",
    "print('one hot 标签：',y_train[1,:],y_train[2,:],y_train[3,:],y_train[4,:])\n",
    "print('对应的实际标签：',np.argmax(y_train[1,:]),np.argmax(y_train[2,:]),np.argmax(y_train[3,:]),np.argmax(y_train[4,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "167abb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_size, output_size, weight_init_std):\n",
    "    \"\"\"\n",
    "    @param input_size:输入向量维度\n",
    "    @param hidden_size:中间神经元个数\n",
    "    @param output_size:输出层神经元个数\n",
    "    @param weight_init_sta:比例因子\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    params = {}\n",
    "\n",
    "    params['W1'] = np.random.randn(input_size,hidden_size) * weight_init_std\n",
    "    params['b1'] = np.zeros((hidden_size,)) #请参考样例完成代码\n",
    "    ### START CODE HERE ### \n",
    "    params['W2'] = np.random.randn(hidden_size,hidden_size) * weight_init_std\n",
    "    params['b2'] = np.zeros((hidden_size,))\n",
    "    params['W3'] = np.random.randn(hidden_size,output_size) * weight_init_std\n",
    "    params['b3'] = np.zeros((output_size,))\n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    print(\"W1's shape:\",params['W1'].shape)\n",
    "    print(\"b1's shape:\",params['b1'].shape)\n",
    "    print(\"W2's shape:\",params['W2'].shape)\n",
    "    print(\"b2's shape:\",params['b2'].shape)\n",
    "    print(\"W3's shape:\",params['W3'].shape)\n",
    "    print(\"b3's shape:\",params['b3'].shape) #请在调用该函数的地方观察该神经网络各个参数的shape，是否符合预期\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e31daf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.  , -0.05],\n",
       "        [-0.2 ,  3.  ]]),\n",
       " array([[ 1.  , -0.05],\n",
       "        [-0.2 ,  3.  ]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LeakyRelu:\n",
    "    def __init__(self):\n",
    "        self.mask = None \n",
    "        self.alpha = 0.1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0) #mask表示选择出x的值中小于等于0的部分内容\n",
    "        out = x.copy()\n",
    "        ### START CODE HERE ###  #请参考LeakyRelu表达式实现前向传播过程\n",
    "        out[self.mask] = out[self.mask]*self.alpha\n",
    "        ### END CODE HERE ### \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        ### START CODE HERE ###  #请参考LeakyRelu表达式y关于x的导数公式实现反向传播过程\n",
    "        dout[self.mask] = dout[self.mask]*self.alpha\n",
    "        ### END CODE HERE ### \n",
    "        dx = dout\n",
    "        return dx\n",
    "leakyRelu = LeakyRelu()\n",
    "x = np.array( [[1.0, -0.5], [-2.0, 3.0]] )\n",
    "leakyRelu.forward(x), leakyRelu.backward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f56b2b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W =W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 权重和偏置参数的导数\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "        ### START CODE HERE ### \n",
    "        out = np.dot(self.x,self.W)+self.b\n",
    "        ### END CODE HERE ### \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        ### START CODE HERE ### \n",
    "        self.dW = np.dot(self.x.T,dout)\n",
    "        self.db = np.sum(dout)\n",
    "        ### END CODE HERE ### \n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 还原输入数据的形状（对应张量）\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7bd16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x.T\n",
    "    x = x - np.max(x, axis=0)\n",
    "    y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    return y.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8efcf598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(pred, y):\n",
    "    if pred.ndim == 1:\n",
    "        y = y.reshape(1, y.size)\n",
    "        pred = pred.reshape(1, pred.size)\n",
    "        \n",
    "    # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引\n",
    "    if y.size == pred.size:\n",
    "        y = y.argmax(axis=1)\n",
    "             \n",
    "    batch_size = pred.shape[0]\n",
    "    \n",
    "    res = None\n",
    "\n",
    "    res = -np.sum(np.log(pred[:, y] + 1e-7)) / batch_size\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d7091da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.pred = None # softmax的输出\n",
    "        self.y = None # 监督数据\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.y = y\n",
    "        self.pred = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.pred, self.y)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.y.shape[0]\n",
    "        if self.y.size == self.pred.size: # 监督数据是one-hot-vector的情况\n",
    "            dx = (self.pred - self.y) / batch_size\n",
    "        else:\n",
    "            dx = self.pred.copy()\n",
    "            dx[np.arange(batch_size), self.y] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4fa79c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        \n",
    "        # 初始化权重\n",
    "        self.params = initialize_parameters(input_size, hidden_size, output_size, weight_init_std)\n",
    "        # 记录训练次数 adam里要用\n",
    "        self.t = 0\n",
    "\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['LeakyRelu1'] = LeakyRelu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['LeakyRelu2'] = LeakyRelu()\n",
    "        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n",
    "    \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # 前向传播\n",
    "        pred = x.copy()\n",
    "        for layer in self.layers.values():\n",
    "            # 通过forward函数完成前向传播\n",
    "            ### START CODE HERE ###\n",
    "            pred = layer.forward(pred) #对每一层进行前向传播预测结果\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "        return pred\n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        # 计算交叉熵损失\n",
    "        ### START CODE HERE ### \n",
    "        pred = self.predict(x) #计算关于x的预测结果\n",
    "        loss = self.lastLayer.forward(pred,y) #使用SoftmaxWithLoss层计算预测结果和y之间的交叉熵损失\n",
    "        ### END CODE HERE ### \n",
    "        return loss\n",
    "    \n",
    "    def accuracy(self, x, y):\n",
    "        # 输入数据x和标签y，输出当前神经网络的预测准确率\n",
    "        accuracy = None\n",
    "        pred = self.predict(x)\n",
    "        pred = np.argmax(pred, axis=1)\n",
    "        if y.ndim != 1:\n",
    "            y = np.argmax(y, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(pred == y) / float(x.shape[0])\n",
    "\n",
    "        return accuracy\n",
    "        \n",
    "    def gradient(self, x, y):\n",
    "        # 前向传播\n",
    "        self.loss(x, y)\n",
    "\n",
    "        # 反向传播\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b064799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(network, grads, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    使用梯度下降法更新network的参数\n",
    "    \"\"\"\n",
    "\n",
    "    #在这里我们给出了最基础的梯度下降法更新网络参数的实现代码，请同学们参考并完成其他优化算法的代码\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        network.params[key] -= learning_rate * grads[key]  #在network现在的参数基础上减去学习率*梯度\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57b1c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, update_params_method, iters_num, train_size, batch_size, learning_rate):\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "    for i in range(iters_num):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x_batch = x_train[batch_mask]\n",
    "        t_batch = y_train[batch_mask]\n",
    "        network.t += 1\n",
    "\n",
    "        # 计算梯度\n",
    "        grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "        # 更新梯度\n",
    "        update_params_method(network, grad, learning_rate)\n",
    "\n",
    "        loss = network.loss(x_batch, t_batch)\n",
    "        train_loss_list.append(loss)\n",
    "\n",
    "        if i % iter_per_epoch == 0:\n",
    "            train_acc = network.accuracy(x_train, y_train)\n",
    "            test_acc = network.accuracy(x_test, y_test)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            print(\"Train acc:{:<.6f}\\tTest acc:{:<.6f}\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9771fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist(path, normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64e8c4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1's shape: (784, 300)\n",
      "b1's shape: (300,)\n",
      "W2's shape: (300, 300)\n",
      "b2's shape: (300,)\n",
      "W3's shape: (300, 10)\n",
      "b3's shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "# 定义神经网络\n",
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f61e103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:0.087500\tTest acc:0.086200\n",
      "Train acc:0.897267\tTest acc:0.901800\n",
      "Train acc:0.928883\tTest acc:0.927000\n",
      "Train acc:0.950750\tTest acc:0.948400\n",
      "Train acc:0.961783\tTest acc:0.957000\n",
      "Train acc:0.971233\tTest acc:0.965700\n",
      "Train acc:0.975550\tTest acc:0.968100\n",
      "Train acc:0.979517\tTest acc:0.970800\n",
      "Train acc:0.981550\tTest acc:0.972300\n",
      "Train acc:0.984417\tTest acc:0.973500\n",
      "Train acc:0.987367\tTest acc:0.977900\n",
      "Train acc:0.988967\tTest acc:0.976800\n",
      "Train acc:0.988100\tTest acc:0.976000\n",
      "Train acc:0.990667\tTest acc:0.977200\n",
      "Train acc:0.990433\tTest acc:0.976100\n",
      "Train acc:0.992433\tTest acc:0.975700\n",
      "Train acc:0.991883\tTest acc:0.977300\n"
     ]
    }
   ],
   "source": [
    "iters_num = 10000 #迭代次数\n",
    "train_size = x_train.shape[0] #训练集的样本数量\n",
    "batch_size = 100 #batch大小\n",
    "learning_rate = 0.1 #学习率\n",
    "train_network(network, update_parameters, iters_num, train_size, batch_size, learning_rate) #开始训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "720d68ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1's shape: (784, 300)\n",
      "b1's shape: (300,)\n",
      "W2's shape: (300, 300)\n",
      "b2's shape: (300,)\n",
      "W3's shape: (300, 10)\n",
      "b3's shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "def initialize_grads_squared(network):\n",
    "    \"\"\"\n",
    "    初始化历史梯度和\n",
    "    \"\"\"\n",
    "    grads_squared = {}\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        \n",
    "        grads_squared[key] = np.zeros(network.params[key].shape)\n",
    "        \n",
    "    return grads_squared\n",
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "grads_squared = initialize_grads_squared(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e0f6f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:0.305933\tTest acc:0.303800\n",
      "Train acc:0.868383\tTest acc:0.870600\n",
      "Train acc:0.885783\tTest acc:0.887900\n",
      "Train acc:0.894000\tTest acc:0.895600\n",
      "Train acc:0.898633\tTest acc:0.899000\n",
      "Train acc:0.902567\tTest acc:0.902600\n",
      "Train acc:0.905517\tTest acc:0.905000\n",
      "Train acc:0.908133\tTest acc:0.908300\n",
      "Train acc:0.909067\tTest acc:0.910300\n",
      "Train acc:0.912300\tTest acc:0.911700\n",
      "Train acc:0.914200\tTest acc:0.913700\n",
      "Train acc:0.916000\tTest acc:0.915000\n",
      "Train acc:0.916933\tTest acc:0.916000\n",
      "Train acc:0.918133\tTest acc:0.916300\n",
      "Train acc:0.919267\tTest acc:0.917900\n",
      "Train acc:0.920100\tTest acc:0.919400\n",
      "Train acc:0.920833\tTest acc:0.920400\n"
     ]
    }
   ],
   "source": [
    "def update_parameters_with_adagrad(network, grads, learning_rate=0.001, epsilon = 1e-7):\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        #计算历史梯度平方和\n",
    "        grads_squared[key] += grads[key]*grads[key] \n",
    "        network.params[key] -= learning_rate*grads[key]/(epsilon+pow(grads_squared[key],0.5))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return \n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_adagrad, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "835bd4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1's shape: (784, 300)\n",
      "b1's shape: (300,)\n",
      "W2's shape: (300, 300)\n",
      "b2's shape: (300,)\n",
      "W3's shape: (300, 10)\n",
      "b3's shape: (10,)\n",
      "Train acc:0.160150\tTest acc:0.151200\n",
      "Train acc:0.962000\tTest acc:0.956500\n",
      "Train acc:0.972233\tTest acc:0.964300\n",
      "Train acc:0.980500\tTest acc:0.970400\n",
      "Train acc:0.984333\tTest acc:0.973800\n",
      "Train acc:0.987800\tTest acc:0.974700\n",
      "Train acc:0.990583\tTest acc:0.976700\n",
      "Train acc:0.991250\tTest acc:0.974700\n",
      "Train acc:0.993517\tTest acc:0.978700\n",
      "Train acc:0.994333\tTest acc:0.977600\n",
      "Train acc:0.994167\tTest acc:0.978100\n",
      "Train acc:0.995850\tTest acc:0.979700\n",
      "Train acc:0.994717\tTest acc:0.979600\n",
      "Train acc:0.994617\tTest acc:0.977800\n",
      "Train acc:0.996633\tTest acc:0.979800\n",
      "Train acc:0.993400\tTest acc:0.975400\n",
      "Train acc:0.996983\tTest acc:0.978800\n"
     ]
    }
   ],
   "source": [
    "def update_parameters_with_rmsprop(network, grads, learning_rate=0.001, epsilon = 1e-7, beta=0.999):\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        #公式里的u就是这里的 grads_squared         \n",
    "        grads_squared[key] = grads_squared[key]*beta+(1-beta)*grads[key]*grads[key]\n",
    "        network.params[key] -=  learning_rate*grads[key]/(epsilon+pow(grads_squared[key],0.5))\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return \n",
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "grads_squared = initialize_grads_squared(network)\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_rmsprop, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59562d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1's shape: (784, 300)\n",
      "b1's shape: (300,)\n",
      "W2's shape: (300, 300)\n",
      "b2's shape: (300,)\n",
      "W3's shape: (300, 10)\n",
      "b3's shape: (10,)\n",
      "Train acc:0.085100\tTest acc:0.083500\n",
      "Train acc:0.439733\tTest acc:0.446900\n",
      "Train acc:0.320867\tTest acc:0.320500\n",
      "Train acc:0.679117\tTest acc:0.684800\n",
      "Train acc:0.777233\tTest acc:0.783700\n",
      "Train acc:0.836733\tTest acc:0.838100\n",
      "Train acc:0.864183\tTest acc:0.865700\n",
      "Train acc:0.878183\tTest acc:0.878600\n",
      "Train acc:0.886017\tTest acc:0.887400\n",
      "Train acc:0.893483\tTest acc:0.893000\n",
      "Train acc:0.898200\tTest acc:0.898800\n",
      "Train acc:0.902217\tTest acc:0.903600\n",
      "Train acc:0.905417\tTest acc:0.906300\n",
      "Train acc:0.907967\tTest acc:0.908000\n",
      "Train acc:0.911500\tTest acc:0.912200\n",
      "Train acc:0.914483\tTest acc:0.914000\n",
      "Train acc:0.916300\tTest acc:0.915200\n"
     ]
    }
   ],
   "source": [
    "def initialize_velocity(network):\n",
    "    v = {}\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        v[key] = np.zeros((network.params[key]).shape) \n",
    "    return v\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "v = initialize_velocity(network)\n",
    "\n",
    "def update_parameters_with_momentum(network, grads, learning_rate=0.001, beta=0.9):\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        #公式里的u就是这里的 grads_squared         \n",
    "        v[key] = beta*v[key]+learning_rate*grads[key]\n",
    "        network.params[key] -= v[key]\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return \n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_momentum, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "faf473aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1's shape: (784, 300)\n",
      "b1's shape: (300,)\n",
      "W2's shape: (300, 300)\n",
      "b2's shape: (300,)\n",
      "W3's shape: (300, 10)\n",
      "b3's shape: (10,)\n",
      "Train acc:0.085150\tTest acc:0.083700\n",
      "Train acc:0.437983\tTest acc:0.445100\n",
      "Train acc:0.321867\tTest acc:0.321200\n",
      "Train acc:0.681133\tTest acc:0.686600\n",
      "Train acc:0.777883\tTest acc:0.785300\n",
      "Train acc:0.836783\tTest acc:0.839300\n",
      "Train acc:0.864383\tTest acc:0.865900\n",
      "Train acc:0.878633\tTest acc:0.879300\n",
      "Train acc:0.886133\tTest acc:0.887800\n",
      "Train acc:0.893383\tTest acc:0.893100\n",
      "Train acc:0.898083\tTest acc:0.898900\n",
      "Train acc:0.902117\tTest acc:0.903100\n",
      "Train acc:0.905250\tTest acc:0.906300\n",
      "Train acc:0.907967\tTest acc:0.907700\n",
      "Train acc:0.911567\tTest acc:0.911800\n",
      "Train acc:0.914217\tTest acc:0.913800\n",
      "Train acc:0.916317\tTest acc:0.914900\n"
     ]
    }
   ],
   "source": [
    "def update_parameters_with_nesterov_momentum(network, grads, learning_rate=0.001, beta=0.9):\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "                \n",
    "        v[key] = beta*v[key]+learning_rate*grads[key]\n",
    "        network.params[key] -= beta*v[key]+learning_rate*grads[key]\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return \n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "v = initialize_velocity(network)\n",
    "\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_nesterov_momentum, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d42ddc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1's shape: (784, 300)\n",
      "b1's shape: (300,)\n",
      "W2's shape: (300, 300)\n",
      "b2's shape: (300,)\n",
      "W3's shape: (300, 10)\n",
      "b3's shape: (10,)\n",
      "Train acc:0.305933\tTest acc:0.303800\n",
      "Train acc:0.958500\tTest acc:0.953100\n",
      "Train acc:0.970083\tTest acc:0.965800\n",
      "Train acc:0.979383\tTest acc:0.970100\n",
      "Train acc:0.985017\tTest acc:0.973700\n",
      "Train acc:0.988350\tTest acc:0.975300\n",
      "Train acc:0.988383\tTest acc:0.974200\n",
      "Train acc:0.992500\tTest acc:0.978200\n",
      "Train acc:0.992250\tTest acc:0.977700\n",
      "Train acc:0.992400\tTest acc:0.978000\n",
      "Train acc:0.993750\tTest acc:0.979200\n",
      "Train acc:0.991567\tTest acc:0.976900\n",
      "Train acc:0.993167\tTest acc:0.977900\n",
      "Train acc:0.996050\tTest acc:0.979100\n",
      "Train acc:0.995133\tTest acc:0.980000\n",
      "Train acc:0.996617\tTest acc:0.979700\n",
      "Train acc:0.994533\tTest acc:0.977200\n"
     ]
    }
   ],
   "source": [
    "def initialize_adam(network) :\n",
    "    v = {}\n",
    "    u = {}\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        v[key] = np.zeros(np.shape(network.params[key]))\n",
    "        u[key] = np.zeros(np.shape(network.params[key]))\n",
    "            \n",
    "    return v, u\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "v, u = initialize_adam(network)\n",
    "\n",
    "def update_parameters_with_adam(network, grads, learning_rate=0.001, epsilon=1e-7, beta1=0.9, beta2=0.999):\n",
    "    v_corrected = {}\n",
    "    u_corrected = {} \n",
    "    t = network.t #当前迭代次数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "                \n",
    "        v[key] = beta1*v[key]+(1-beta1)*grads[key]\n",
    "        v_corrected[key] = v[key]/(1-pow(beta1,t))\n",
    "        \n",
    "        u[key] = beta2*u[key]+(1-beta2)*grads[key]*grads[key]\n",
    "        u_corrected[key] = u[key]/(1-pow(beta2,t))\n",
    "        \n",
    "        network.params[key] -= learning_rate*v_corrected[key]/(pow(u_corrected[key],0.5)+epsilon)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return \n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_adam, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f27f892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1's shape: (784, 300)\n",
      "b1's shape: (300,)\n",
      "W2's shape: (300, 300)\n",
      "b2's shape: (300,)\n",
      "W3's shape: (300, 10)\n",
      "b3's shape: (10,)\n",
      "Train acc:0.313700\tTest acc:0.311900\n",
      "Train acc:0.960467\tTest acc:0.955400\n",
      "Train acc:0.970850\tTest acc:0.965300\n",
      "Train acc:0.981183\tTest acc:0.970400\n",
      "Train acc:0.985850\tTest acc:0.974900\n",
      "Train acc:0.988117\tTest acc:0.974900\n",
      "Train acc:0.989167\tTest acc:0.974400\n",
      "Train acc:0.992033\tTest acc:0.978700\n",
      "Train acc:0.994133\tTest acc:0.978600\n",
      "Train acc:0.993683\tTest acc:0.977800\n",
      "Train acc:0.992567\tTest acc:0.976200\n",
      "Train acc:0.990833\tTest acc:0.976800\n",
      "Train acc:0.996383\tTest acc:0.980100\n",
      "Train acc:0.996583\tTest acc:0.978900\n",
      "Train acc:0.996083\tTest acc:0.979000\n",
      "Train acc:0.995433\tTest acc:0.978400\n",
      "Train acc:0.994850\tTest acc:0.977800\n"
     ]
    }
   ],
   "source": [
    "def initialize_adambelief(network) :\n",
    "    v = {}\n",
    "    s = {}\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ###  #请初始化v和s\n",
    "        v[key] = np.zeros(np.shape(network.params[key]))\n",
    "        s[key] = np.zeros(np.shape(network.params[key]))\n",
    "        ### END CODE HERE ###\n",
    "            \n",
    "    return v, s\n",
    "\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "v, s = initialize_adambelief(network)\n",
    "\n",
    "def update_parameters_with_adambelief(network, grads, learning_rate=0.001, epsilon=1e-7, beta1=0.9, beta2=0.999):\n",
    "    v_corrected = {}\n",
    "    s_corrected = {} \n",
    "    t = network.t #当前迭代次数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "                \n",
    "        v[key] = beta1*v[key]+(1-beta1)*grads[key]\n",
    "        v_corrected[key] = v[key]/(1-pow(beta1,t))\n",
    "        \n",
    "        s[key] = beta2*s[key]+(1-beta2)*pow(grads[key]-v[key],2)\n",
    "        s_corrected[key] = s[key]/(1-pow(beta2,t))\n",
    "        \n",
    "        network.params[key] -= learning_rate*v_corrected[key]/(epsilon+pow(s_corrected[key],0.5))\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return \n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_adambelief, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1864c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
